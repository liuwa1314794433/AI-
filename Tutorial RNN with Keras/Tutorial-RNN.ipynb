{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMM7370 AI Theories and Applications\n",
    "# Tutorial: Recurrent Neural Network by Keras\n",
    "## The Problem: Movie review sentiment analysis\n",
    "In this tutorial, we will perform sentiment analysis on a corpus of movie reviews from Rotten Tomatoes. \n",
    "<img src=\"tomato.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The dataset is from [Kaggle](https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data). In this tutorial, we use 39015 phrases as training data and 7803 phrases as testing data. Each phrase is labeled on a scale of zero to four. The sentiment corresponding to each of the labels are:\n",
    "- 0: negative\n",
    "- 1: somewhat negative\n",
    "- 2: neutral\n",
    "- 3: somewhat positive\n",
    "- 4: positive\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install used packages in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# If you are using MacOS, please un-comment the following line\n",
    "# allow to duplicate dll\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('train_sentiment.csv')\n",
    "df_test = pd.read_csv('test_sentiment.csv')\n",
    "\n",
    "y_train = df_train['Sentiment']\n",
    "y_test = df_test['Sentiment']\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see some phrases are incomplete and some repeat.\n",
    "### Clean text\n",
    "ASCII characters are ultimately interpreted by the computer as hexadecimal. In consequence, to a computer, ‘A’ is not the same as ‘a’. Therefore, we’ll want to change all characters to lowercase. Since we’re going to be splitting the sentences up into individual words based on white spaces, a word with a period right after it is not equivalent to one without a period following it (*happy. != happy*). In addition, contractions are going to be interpreted differently than the original which will have repercussions for the model (*I’m != I am*). Thus, we replace all occurrences using the proceeding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let’s\": 'let us',\n",
    "                r\"'s\":  ' is',\n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan’t\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',\n",
    "                ',': ' ,',\n",
    "                '.': ' .',\n",
    "                '!': ' !',\n",
    "                '?': ' ?',\n",
    "                '\\s+': ' '}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `apply` method to apply the function to every row in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['Phrase'].apply(lambda p: clean_text(p))\n",
    "X_test = df_test['Phrase'].apply(lambda p: clean_text(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lamda`: A lambda function is a small anonymous function.  \n",
    "`Dataframe.apply()` calls the passed lambda function for each row and passes each row contents as series to this lambda function. Finally it returns a modified copy of dataframe constructed with rows returned by lambda functions, instead of altering original dataframe.\n",
    "\n",
    "Let’s look at the individual length of each phrase in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_len = X_train.apply(lambda p: len(p.split(' ')))\n",
    "max_phrase_len = phrase_len.max()\n",
    "print('max phrase len: {0}'.format(max_phrase_len))\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.hist(phrase_len, alpha = 0.2, density = True)\n",
    "plt.xlabel('phrase len')\n",
    "plt.ylabel('probability')\n",
    "plt.grid(alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_len = X_test.apply(lambda p: len(p.split(' ')))\n",
    "if max_phrase_len < phrase_len.max():\n",
    "    max_phrase_len = phrase_len.max()\n",
    "print('max phrase len: {0}'.format(max_phrase_len))\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.hist(phrase_len, alpha = 0.2, density = True)\n",
    "plt.xlabel('phrase len')\n",
    "plt.ylabel('probability')\n",
    "plt.grid(alpha = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the inputs to a neural network must be the same length. Therefore, we store the longest length as a variable which we’ll use later to define the input to our model.\n",
    "\n",
    "### Word embedding\n",
    "Computers don’t understand words, let alone sentences, therefore, we use the tokenizer to parse the phrases. In specifying `num_words`, only the most common words will be kept.   \n",
    "The tokens are then vectorized. By vectorized we mean that they are mapped to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 8192\n",
    "tokenizer = Tokenizer(\n",
    "    num_words = max_words,\n",
    "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~'\n",
    ")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer`: [Tokenizer](https://keras.io/preprocessing/text/) is the text tokenization utility class. This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector\n",
    "\n",
    "`fit_on_texts`: Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word \n",
    "\n",
    "`texts_to_sequences`: Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "\n",
    "### Pad sequences\n",
    "In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this using the `pad_sequences()` function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_phrase_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Model\n",
    "Our model is a simple RNN model with 1 embedding, 1 LSTM and 1 dense layers.\n",
    "<img src=\"network.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_words, output_dim = 100, input_length = max_phrase_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Embedding`: [Embedding](https://keras.io/layers/embeddings/) layer turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. This layer can only be used as the first layer in a model. \n",
    "    - input_dim: int > 0. Size of the vocabulary\n",
    "    - output_dim: int >= 0. Dimension of the dense embedding.\n",
    "    - input_length: Length of input sequences, when it is constant. \n",
    "- [`LSTM`](https://keras.io/layers/recurrent/): Long Short-Term Memory layer\n",
    "- `Dropout` consists in randomly setting a fraction rate of input units, indicates the fraction of the input units to drop at each update during training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compiling the Model\n",
    "Before we can begin training, we need to configure the training process. We decide 3 key factors during the compilation step:\n",
    "- The **optimizer**. We’ll stick with a pretty good default: the Adam gradient-based optimizer (Adam - A Method for Stochastic Optimization). Keras has many [other optimizers](https://keras.io/optimizers/) you can look into as well.\n",
    "- The **loss function**. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss. Keras distinguishes between binary_crossentropy (2 classes) and categorical_crossentropy (>2 classes), so we’ll use the latter. [See all Keras losses](https://keras.io/losses/).\n",
    "- A list of **metrics**. Since this is a classification problem, we’ll just have Keras report on the accuracy metric.\n",
    "\n",
    "Here’s what that compilation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    to_categorical(y_train),\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data=(X_test, to_categorical(y_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the Model\n",
    "Now that we have a working, trained model, let’s put it to use. The first thing we’ll do is save it to disk so we can load it back up anytime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now reload the trained model whenever we want by rebuilding it and loading in the saved weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_words, output_dim = 100, input_length = max_phrase_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "# Load the model from disk later using:\n",
    "model.load_weights('rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction result on test data\n",
    "Using the trained model to make predictions is easy: we pass an array of inputs to `predict()` and it returns an array of outputs. Keep in mind that the output of our network are probabilities (because of softmax), so we’ll use `np.argmax()` to turn those into actual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the first 5 test sequences.\n",
    "predictions = model.predict(X_test[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "# If you are using MacOS, please un-comment the following line\n",
    "# allow to duplicate dll\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_csv('train_sentiment.csv')\n",
    "df_test = pd.read_csv('test_sentiment.csv')\n",
    "\n",
    "y_train = df_train['Sentiment']\n",
    "y_test = df_test['Sentiment']\n",
    "\n",
    "# clean data\n",
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let’s\": 'let us',\n",
    "                r\"'s\":  ' is',\n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan’t\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',\n",
    "                ',': ' ,',\n",
    "                '.': ' .',\n",
    "                '!': ' !',\n",
    "                '?': ' ?',\n",
    "                '\\s+': ' '}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "X_train = df_train['Phrase'].apply(lambda p: clean_text(p))\n",
    "X_test = df_test['Phrase'].apply(lambda p: clean_text(p))\n",
    "\n",
    "phrase_len = X_train.apply(lambda p: len(p.split(' ')))\n",
    "max_phrase_len = phrase_len.max()\n",
    "phrase_len = X_test.apply(lambda p: len(p.split(' ')))\n",
    "if max_phrase_len < phrase_len.max():\n",
    "    max_phrase_len = phrase_len.max()\n",
    "\n",
    "# word embedding\n",
    "max_words = 8192\n",
    "tokenizer = Tokenizer(\n",
    "    num_words = max_words,\n",
    "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~'\n",
    ")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# pad sequence\n",
    "X_train = pad_sequences(X_train, maxlen = max_phrase_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_phrase_len)\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_words, output_dim = 100, input_length = max_phrase_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    to_categorical(y_train),\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data=(X_test, to_categorical(y_test))\n",
    ")\n",
    "\n",
    "model.save_weights('rnn.h5')\n",
    "# Load the model from disk later using:\n",
    "#model.load_weights('rnn.h5')\n",
    "\n",
    "# Predict on the first 5 test sequences.\n",
    "predictions = model.predict(X_test[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The codes in this notebook are modified from various sources. All codes are for educational purposes only and released under the CC1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
