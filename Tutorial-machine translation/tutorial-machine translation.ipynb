{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMM7370 AI Theories and Applications\n",
    "# Tutorial: Machine Translation by Keras\n",
    "## The Problem: Chinese-English translation\n",
    "In this tutorial, we'll build a simple Chinese-English translation model. \n",
    "### Seq2Seq model\n",
    "A simple Seq2Seq model consists of three parts, Encoder-LSTM, Decoder-LSTM, and Context.The input sequence is ABC, and Encoder-LSTM processes the input sequence and returns the hidden state of the entire input sequence in the last neuron, also known as the context (C).Decoder-LSTM then predicts the next character of the target sequence step by step based on the hidden state.The final output sequence wxyz.It is worth mentioning that the author Sutskever designed a specific Seq2Seq model based on his specific tasks.The input sequence is processed in reverse order, which enables the model to process long sentences and improves the accuracy.\n",
    "<img src=\"s2s.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    "### Actural model\n",
    "<img src=\"actural.jpg\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "The above image is a real model designed by the Sutskever in [this paper](https://arxiv.org/pdf/1409.3215.pdf).\n",
    "\n",
    "The input to the encoder LSTM is the sentence in the original language; the input to the decoder LSTM is the sentence in the translated language with a `start-of-sentence` token. The output is the actual target sentence with an `end-of-sentence` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install used packages in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for macOS\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "### Dataset\n",
    "The language translation model that we are going to develop will translate English sentences into their Chinese language counterparts. To develop such a model, we need a dataset that contains English sentences and their Chinese translations. This dataset is from http://www.manythings.org/anki/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "num_data = 100\n",
    "data = data[:num_data]\n",
    "print(data[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each line, the text file contains an English sentence and its Chinese translation, separated by a tab. Each line also contains some other information that not related to our translation model and will be omitted.\n",
    "\n",
    "The dataset contains 22,075 records, but we will use first `num_data` records (which equals to 100 currently) to train our model. You can use more records if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "In our dataset, we do not need to process the input, however, we need to generate two copies of the translated sentence: one with the start-of-sentence token and the other with the end-of-sentence token.\n",
    "- start-of-sentence token: '\\t'\n",
    "- end-of-sentence token: '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_data = [line.split('\\t')[0] for line in data]\n",
    "ch_data = ['\\t' + line.split('\\t')[1] + '\\n' for line in data]\n",
    "print('English data:\\n', en_data[:10])\n",
    "print('\\nChinese data:\\n', ch_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `Tokenizer` class doesn't support Chinese documents well, in this tutorial, we manually generate English&Chinese dictionaries and use one-hot encoding to embed words. The following script:\n",
    "- Generate English&Chinese dictionaries\n",
    "- Map each character into an index\n",
    "\n",
    "[`enumerate()`](https://www.geeksforgeeks.org/enumerate-in-python/) method adds a counter to an iterable and returns it in a form of `enumerate object`. This enumerate object can then be used directly in for loops or be converted into a list of tuples using list() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate English dictionary\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "# generate Chinese dictionary \n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "print('\\n English dictionary:\\n', en2id)\n",
    "print('\\n Chinese dictionary\\n:', ch2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word embedding and padding\n",
    "Map data sample according to dictionary index.\n",
    "- `en_num_data` - encoder input data\n",
    "- `ch_num_data` - decoder input data (with sos)\n",
    "- `de_num_data` - decoder target output data (with eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line][:-1] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
    "\n",
    "print('char:', en_data[1])\n",
    "print('index:', en_num_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max encoder/decoder sequence length \n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print('max encoder length:', max_encoder_seq_length)\n",
    "print('max decoder length:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script encodes the English/Chinese sentences with one-hot encoding, and padding the sentences into the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(de_num_data), max_decoder_seq_length, len(ch2id)), dtype='float32')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print('index data:\\n', en_num_data[1])\n",
    "print('one hot data:\\n', encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Model\n",
    "Set values for different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the encoder and decoders. \n",
    "### Encoder\n",
    "The input to the encoder will be the sentence in English and the output will be the *hidden state* and *cell state* of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, EN_VOCAB_SIZE))\n",
    "encoder_lstm = LSTM(HIDDEN_SIZE, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# We discard encoder_outputs and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Input()`](https://keras.io/layers/core/#Input) - used to instantiate a Keras tensor.\n",
    "- shape: A shape tuple (integer), not including the batch size. Here, encoder_inputs is a 1\\*EN_VOCAB_SIZE tensor.\n",
    "\n",
    "`LSTM` network\n",
    "- return_state: Boolean. Whether to return the last state in addition to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an <sos> token appended at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
    "decoder_lstm = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "\n",
    "# obtain output\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return_sequences: Boolean. This argument tells Whether to return the output at each time steps instead of the final time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output from the decoder LSTM is passed through a dense layer to predict decoder outputs, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Model`](https://keras.io/models/model/) - given some input tensor(s) and output tensor(s), you can instantiate a Model via `Model()`. This model will include all layers required in the computation of outputs given inputs.\n",
    "\n",
    "[`Adam`]() optimizer\n",
    "- beta_1 - The exponential decay rate for the first moment estimates (e.g. 0.9).\n",
    "- beta_2 - The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "- epsilon - Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data[0:int(num_data*0.9)], decoder_input_data[0:int(num_data*0.9)]], decoder_target_data[0:int(num_data*0.9)],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the trained model to disk so we can load it back up anytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model #model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modifying the Model for Predictions\n",
    "While training, we know the actual inputs to the decoder for all the output words in the sequence. The input to the decoder and output from the decoder is known and the model is trained on the basis of these inputs and outputs.\n",
    "\n",
    "However, during predictions the next word will be predicted on the basis of the previous word, which in turn is also predicted in the previous time-step. Now you will understand the purpose of `sos` and `eos` tokens. While making actual predictions, the full output sequence is not available, in fact that is what we have to predict. During prediction the only word available to us is `sos` since all the output sentences start with `sos`.\n",
    "\n",
    "The encoder model remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now at each step we need the decoder hidden and cell states, we will modify our model to accept the hidden and cell states as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, the decoder output is passed through the dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to define the updated decoder model, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make predictions\n",
    "In this step, you will see how to make predictions using English sentences as inputs.\n",
    "\n",
    "We pass the test input sequence to the `encoder_model`, which predicts the hidden state `h` and the cell state `c`.\n",
    "\n",
    "Next, we define a variable `target_seq`, which is a `1 * 1` matrix of all zeros. The `target_seq` variable contains the first word to the decoder model, which is `sos`.\n",
    "\n",
    "In the next line, the `outputs` list is defined, which will contain the predicted translation.\n",
    "\n",
    "Next, we execute a while loop.  \n",
    "- Inside the loop, in the first iteration, the `decoder_model` predicts the output and the hidden and cell states, using the hidden and cell state of the encoder, and the input token, i.e. `sos`. The index of the predicted word is stored in `sampled_token_index`. The predicted index is then appended to the `outputs` list. The index of the predicted word is stored in the target_seq variable. In the next loop cycle, the updated hidden and cell states, along with the index of the previously predicted word, are used to make new predictions. The loop continues until the maximum output sequence length is achieved or the `eos` token is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(int(num_data*0.9),num_data):\n",
    "    test_data = encoder_input_data[k:k+1]\n",
    "    # Encode the input as state vectors.\n",
    "    h, c = encoder_model.predict(test_data)\n",
    "    target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "    target_seq[0, 0, ch2id['\\t']] = 1\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, h, c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        outputs.append(sampled_token_index)\n",
    "        target_seq = np.zeros((1, 1, CH_VOCAB_SIZE))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        if sampled_token_index == ch2id['\\n'] or len(outputs) > 20: \n",
    "            break\n",
    "    \n",
    "    print(en_data[k])\n",
    "    print(''.join([id2ch[i] for i in outputs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The codes in this notebook are modified from various sources. All codes are for educational purposes only and released under the CC1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
